{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning 2: predictive models, deep learning, neural network 2022Z\n",
    "\n",
    "### This material may not be copied or published elsewhere (including Facebook and other social media) without the  permission of the author!\n",
    "\n",
    "\n",
    "# Theano\n",
    "\n",
    "- Python library for defining mathematical functions (operating over vectors and matrices),\n",
    "- allows to compute the gradients of defined functions\n",
    "- low level library for building and training neural networks\n",
    "- the foundational layer for many deep learning packages (ex. Keras,Lasagne)\n",
    "\n",
    "# Instalation of Keras\n",
    "\n",
    "Hint: Check which Anaconda version is suitable for architecture of Your system [32-bit or 64-bit?]\n",
    "<br><br>\n",
    "<b>Using Anaconda Prompt </b><br>\n",
    "1. windows search-> Anaconda Prompt\n",
    "2. conda update conda [enter]\n",
    "3. conda update anaconda [enter]\n",
    "4. conda install theano [enter]\n",
    "5. Proceed y/n y\n",
    "\n",
    "or <br><br>\n",
    "<b>Using Jupyter</b><br>\n",
    "1. !pip install theano \n",
    "\n",
    "### Optimizing loss functions = use of stochastic gradient descent (SGD)\n",
    "\n",
    "Loss functions in deep learning are complicated,it is not convenient to derive the gradients manually. Theano allows to define functions as mathematical expressions and calculate the gradients. <br>\n",
    "\n",
    "A typical workflow for using Theano:\n",
    "\n",
    "1. Define the loss function using symbolic expression\n",
    "2. Calculate the gradients of loss function using Theano\n",
    "3. Pass this gradient function as a parameter to a SGD optimization routine to optimize the loss function\n",
    "\n",
    "The strengths of Theano:\n",
    "- seamless integration with Numpy that allows the user to use Numpy\n",
    "objects (vectors and matrices) in the definition of loss functions\n",
    "- generate optimized code for both CPU as well as GPU\n",
    "- optimized automatic/symbolic differentiation\n",
    "- numerical stability for the generated code via automatic/symbolic differentiation\n",
    "\n",
    "### Check Theano configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theano: 1.0.4\n"
     ]
    }
   ],
   "source": [
    "#install theano\n",
    "#!pip install theano \n",
    "\n",
    "# theano \n",
    "import theano\n",
    "print('theano: %s' % theano.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Function with Scalars\n",
    "\n",
    "1. Scalars are defined before they can be used in a mathematical expression.\n",
    "2. Every scalar is given a unique name.\n",
    "3. Once defined, the scalar can be operated upon with operations like +, -, * and /.\n",
    "4. The function construct in Theano allows one to relate inputs and outputs. \n",
    "We have defined a function with the name g, which takes a, b, c, d, and e as input and produces f as the output.\n",
    "5. We can now compute the result of the function g given the input and check that it\n",
    "evaluates exactly as the non-Theano expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
      "Via Theano: ((1 - 2 + 3) * 4)/5.0 =  1.6\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "from theano import function\n",
    "a = T.dscalar('a')\n",
    "b = T.dscalar('b')\n",
    "c = T.dscalar('c')\n",
    "d = T.dscalar('d')\n",
    "e = T.dscalar('e')\n",
    "f = ((a - b + c) * d )/e\n",
    "g = function([a, b, c, d, e], f)\n",
    "print(\"Expected: ((1 - 2 + 3) * 4)/5.0 = \", ((1 - 2 + 3) * 4)/5.0)\n",
    "print(\"Via Theano: ((1 - 2 + 3) * 4)/5.0 = \", g(1, 2, 3, 4, 5))\n",
    "# Expected: ((1 - 2 + 3) * 4)/5.0 = 1.6\n",
    "# Via Theano: ((1 - 2 + 3) * 4)/5.0 = 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Functions with Vectors\n",
    "\n",
    "1. Vectors/Matrices are defined before they can be used in a mathematical\n",
    "expression.\n",
    "2. Every Vector/Matrix is given a unique name.\n",
    "3. The dimensions of the Vectors/Matrices are not specified.\n",
    "4. Once Vectors/Matrices are defined, the user can define operations like matrix\n",
    "addition, subtraction, and multiplication.\n",
    "5. As before, the user can define a function based on the defined expressions. In\n",
    "this case we define a function f that takes a, b, c, and d as input and produces e as\n",
    "output.\n",
    "6. The user can pass Numpy arrays to the function and compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [[-6 -6]\n",
      " [-6 -6]]\n",
      "Via Theano: [[-6. -6.]\n",
      " [-6. -6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "a = T.dmatrix('a')\n",
    "b = T.dmatrix('b')\n",
    "c = T.dmatrix('c')\n",
    "d = T.dmatrix('d')\n",
    "\n",
    "e = (a + b - c) * d\n",
    "f = function([a,b,c,d], e)\n",
    "a_data = numpy.array([[1,1],[1,1]])\n",
    "b_data = numpy.array([[2,2],[2,2]])\n",
    "c_data = numpy.array([[5,5],[5,5]])\n",
    "d_data = numpy.array([[3,3],[3,3]])\n",
    "print(\"Expected:\", (a_data + b_data - c_data) * d_data)\n",
    "print(\"Via Theano:\", f(a_data,b_data,c_data,d_data))\n",
    "# Expected: [[-6 -6]\n",
    "# [-6 -6]]\n",
    "# Via Theano: [[-6. -6.]\n",
    "# [-6. -6.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions with Scalars and Vectors\n",
    "\n",
    "1. Scalars and vectors/matrices can be used together in expressions.\n",
    "2. The user needs to take care that vector/matrices respect the dimensionality both\n",
    "while defining the expressions as well as passing inputs to the expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [[-26.25 -26.25]\n",
      " [-26.25 -26.25]]\n",
      "Via Theano: [[-26.25 -26.25]\n",
      " [-26.25 -26.25]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "\n",
    "a = T.dmatrix('a')\n",
    "b = T.dmatrix('b')\n",
    "c = T.dmatrix('c')\n",
    "d = T.dmatrix('d')\n",
    "p = T.dscalar('p')\n",
    "q = T.dscalar('q')\n",
    "r = T.dscalar('r')\n",
    "s = T.dscalar('s')\n",
    "u = T.dscalar('u')\n",
    "\n",
    "e = (((a * p) + (b - q) - (c + r )) * d/s) * u\n",
    "\n",
    "f = function([a,b,c,d,p,q,r,s,u], e)\n",
    "a_data = numpy.array([[1,1],[1,1]])\n",
    "b_data = numpy.array([[2,2],[2,2]])\n",
    "c_data = numpy.array([[5,5],[5,5]])\n",
    "d_data = numpy.array([[3,3],[3,3]])\n",
    "print(\"Expected:\", (((a_data * 1.0) + (b_data - 2.0) - (c_data + 3.0 )) * d_data/4.0) * 5.0)\n",
    "print(\"Via Theano:\", f(a_data,b_data,c_data,d_data,1,2,3,4,5))\n",
    "# Expected: [[-26.25 -26.25]\n",
    "# [-26.25 -26.25]]\n",
    "# Via Theano: [[-26.25 -26.25]\n",
    "# [-26.25 -26.25]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example: Activiation Functions\n",
    "\n",
    "1. The nnet package in Theano is used to define a number of common activation functions.\n",
    "2. Activation functions:\n",
    "    - sigmoid\n",
    "    - tanh\n",
    "    - fast_sigmoid \n",
    "    - soft_plus\n",
    "    - relu\n",
    "    - soft_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid: [array([[0.26894142, 0.5       , 0.73105858]])]\n",
      "tanh: [array([[-0.76159416,  0.        ,  0.76159416]])]\n",
      "fast sigmoid: [array([[0.25, 0.5 , 0.75]])]\n",
      "soft plus: [array([[0.31326169, 0.69314718, 1.31326169]])]\n",
      "relu: [array([[0., 0., 1.]])]\n",
      "soft max: [array([[0.09003057, 0.24472847, 0.66524096]])]\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "from theano import function\n",
    "\n",
    "# sigmoid\n",
    "a = T.dmatrix('a')\n",
    "f_a = T.nnet.sigmoid(a)\n",
    "f_sigmoid = function([a],[f_a])\n",
    "print(\"sigmoid:\", f_sigmoid([[-1,0,1]]))\n",
    "\n",
    "# tanh\n",
    "b = T.dmatrix('b')\n",
    "f_b = T.tanh(b)\n",
    "f_tanh = function([b],[f_b])\n",
    "print(\"tanh:\", f_tanh([[-1,0,1]]))\n",
    "\n",
    "# fast sigmoid\n",
    "c = T.dmatrix('c')\n",
    "f_c = T.nnet.ultra_fast_sigmoid(c)\n",
    "f_fast_sigmoid = function([c],[f_c])\n",
    "print(\"fast sigmoid:\", f_fast_sigmoid([[-1,0,1]]))\n",
    "\n",
    "# softplus\n",
    "d = T.dmatrix('d')\n",
    "f_d = T.nnet.softplus(d)\n",
    "f_softplus = function([d],[f_d])\n",
    "print(\"soft plus:\",f_softplus([[-1,0,1]]))\n",
    "# relu\n",
    "e = T.dmatrix('e')\n",
    "f_e = T.nnet.relu(e)\n",
    "f_relu = function([e],[f_e])\n",
    "print(\"relu:\",f_relu([[-1,0,1]]))\n",
    "\n",
    "# softmax\n",
    "f = T.dmatrix('f')\n",
    "f_f = T.nnet.softmax(f)\n",
    "f_softmax = function([f],[f_f])\n",
    "print(\"soft max:\",f_softmax([[-1,0,1]]))\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : Shared Variables\n",
    "\n",
    "1. All models (deep learning or otherwise) will involve defining functions with\n",
    "internal state, which will typically be weights that need to be learned or fitted.\n",
    "2. A shared variable is defined using the shared construct in Theano.\n",
    "3. A shared variable can be initialized with Numpy constructs.\n",
    "4. Once the shared variable is defined and initialized, it can be used in the\n",
    "definition of expressions and functions similar to scalars and\n",
    "vectors/matrices.\n",
    "5. A user can get the value of the shared variable using the <b>get_value method</b>.\n",
    "6. A user can set the value for the shared variable using the <b>set_value method</b>.\n",
    "7. A function defined using the shared variable computes its output based on\n",
    "the current value of the shared variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shared Value: [[4 5 6]]\n",
      "Original Function Evaluation: [array([[5., 7., 9.]])]\n",
      "Original Shared Value: [[5 6 7]]\n",
      "Original Function Evaluation: [array([[ 6.,  8., 10.]])]\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "from theano import function\n",
    "from theano import shared\n",
    "import numpy\n",
    "x = T.dmatrix('x')\n",
    "y = shared(numpy.array([[4, 5, 6]]))\n",
    "z = x + y\n",
    "f = function(inputs = [x], outputs = [z])\n",
    "print(\"Original Shared Value:\", y.get_value())\n",
    "print(\"Original Function Evaluation:\", f([[1, 2, 3]]))\n",
    "y.set_value(numpy.array([[5, 6, 7]]))\n",
    "print(\"Original Shared Value:\", y.get_value())\n",
    "print(\"Original Function Evaluation:\", f([[1, 2, 3]]))\n",
    "# Couldn't import dot_parser, loading of dot files will not be possible.\n",
    "# Original Shared Value: [[4 5 6]]\n",
    "# Original Function Evaluation: [array([[ 5., 7., 9.]])]\n",
    "# Original Shared Value: [[5 6 7]]\n",
    "# Original Function Evaluation: [array([[ 6., 8., 10.]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gradients\n",
    "\n",
    "1. A function needs to be defined using expressions before the gradient of the\n",
    "function can be generated.\n",
    "2. The grad construct in Theano allows the user to generate the gradient of a\n",
    "function (as an expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [array(68.)]\n",
      "Original Gradient: [array([[ 7., 17., 33.]])]\n",
      "Updated: [array(42.)]\n",
      "Updated Gradient [array([[ 4., 13., 28.]])]\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "from theano import function\n",
    "from theano import shared\n",
    "import numpy\n",
    "x = T.dmatrix('x')\n",
    "y = shared(numpy.array([[4, 5, 6]]))\n",
    "z = T.sum(((x * x) + y) * x)\n",
    "f = function(inputs = [x], outputs = [z])\n",
    "g = T.grad(z,[x])\n",
    "g_f = function([x], g)\n",
    "\n",
    "print(\"Original:\", f([[1, 2, 3]]))\n",
    "print(\"Original Gradient:\", g_f([[1, 2, 3]]))\n",
    "y.set_value(numpy.array([[1, 1, 1]]))\n",
    "print(\"Updated:\", f([[1, 2, 3]]))\n",
    "print(\"Updated Gradient\", g_f([[1, 2, 3]]))\n",
    "# Original: [array(68.0)]\n",
    "# Original Gradient: [array([[ 7., 17., 33.]])]\n",
    "# Updated: [array(42.0)]\n",
    "# Updated Gradient [array([[ 4., 13., 28.]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before Training: 0.521\n",
      "Accuracy after Training: 0.733\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import sklearn.metrics\n",
    "\n",
    "def l2(x):\n",
    "    return T.sum(x**2)\n",
    "examples = 1000\n",
    "features = 100\n",
    "hidden = 10\n",
    "\n",
    "D = (numpy.random.randn(examples, features), numpy.random.randint(size=examples,low=0, high=2))\n",
    "training_steps = 1000\n",
    "\n",
    "x = T.dmatrix(\"x\")\n",
    "y = T.dvector(\"y\")\n",
    "w1 = theano.shared(numpy.random.randn(features, hidden), name=\"w1\")\n",
    "b1 = theano.shared(numpy.zeros(hidden), name=\"b1\")\n",
    "w2 = theano.shared(numpy.random.randn(hidden), name=\"w2\")\n",
    "b2 = theano.shared(0., name=\"b2\")\n",
    "p1 = T.tanh(T.dot(x, w1) + b1)\n",
    "p2 = T.tanh(T.dot(p1, w2) + b2)\n",
    "prediction = p2 > 0.5\n",
    "\n",
    "error = T.nnet.binary_crossentropy(p2,y)\n",
    "\n",
    "loss = error.mean() + 0.01 * (l2(w1) + l2(w2))\n",
    "\n",
    "gw1, gb1, gw2, gb2 = T.grad(loss, [w1, b1, w2, b2])\n",
    "\n",
    "train = theano.function(inputs=[x,y],outputs=[p2, error], updates=((w1, w1 - 0.1 * gw1),\n",
    "(b1, b1 - 0.1 * gb1), (w2, w2 - 0.1 * gw2), (b2, b2 - 0.1 * gb2)))\n",
    "\n",
    "predict = theano.function(inputs=[x], outputs=[prediction])\n",
    "\n",
    "print(\"Accuracy before Training:\", sklearn.metrics.accuracy_score(D[1], numpy.array(predict(D[0])).ravel()))\n",
    "for i in range(training_steps):\n",
    "    prediction, error = train(D[0], D[1])\n",
    "print(\"Accuracy after Training:\", sklearn.metrics.accuracy_score(D[1],numpy.array(predict(D[0])).ravel()))\n",
    "# Accuracy before Training: 0.51\n",
    "# Accuracy after Training: 0.716\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "- <b>Numerical gradient</b>: slow :(, approximate :(, easy to write :)\n",
    "- <b>Analytic gradient </b>: fast :), exact :), error-prone :(\n",
    "\n",
    "$$\\frac{df(x)}{dx}=lim\\frac{f(x+h)-f(x)}{f(x)}$$\n",
    "\n",
    "\n",
    "## Simple example:\n",
    "<br>\n",
    "<img src=\"backprop_0.png\">\n",
    "\n",
    "\n",
    "## Local gradient \n",
    "<img src=\"backprop_1.png\">\n",
    "\n",
    "\n",
    "## More complex example:\n",
    "\n",
    "<img src=\"backprop_2a.png\">\n",
    "\n",
    "### Step_1\n",
    "<img src=\"backprop_2.png\">\n",
    "\n",
    "### Step_2\n",
    "<img src=\"backprop_3.png\">\n",
    "\n",
    "\n",
    "### Step_3\n",
    "<img src=\"backprop_4.png\">\n",
    "\n",
    "### Step_4\n",
    "<img src=\"backprop_5.png\">\n",
    "\n",
    "### Step_5\n",
    "<img src=\"backprop_6.png\">\n",
    "\n",
    "### Step_6\n",
    "<img src=\"backprop_7.png\">\n",
    "\n",
    "## Alternative approach:\n",
    "<img src=\"backprop_8.png\">\n",
    "<br>\n",
    "<img src=\"backprop_9.png\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.350\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      "[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': -0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': 0.0026279652850863837}]\n",
      "[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': -0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': 0.03803132596437354}]\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "            \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "            \n",
    "#Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\n",
    "#Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "* any modification to a learning algorithm to reduce its generalization error but not its training error\n",
    "* reduce generalization error even at the expense of increasing training error:\n",
    "    - E.g., Limiting model capacity is a regularization method\n",
    "    \n",
    "## Generalization error\n",
    "- performance on inputs not previously seen.(Also called as Test error)\n",
    "    \n",
    "## Goals of regularization\n",
    "1. Encode prior knowledge\n",
    "2. Express preference for simpler model\n",
    "3. Need to make underdetermined problem determined\n",
    "\n",
    "## Methods\n",
    "1. Limiting capacity: no of hidden units\n",
    "2. Norm Penalties: L2- and L1- regularization\n",
    "3. Early stopping\n",
    "\n",
    "<br>\n",
    "<img src=\"regularization_0.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
