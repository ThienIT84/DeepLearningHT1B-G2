{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This material may not be copied or published elsewhere (including Facebook and other social media) without the  permission of the author!\n",
    "\n",
    "##### Machine Learning 2: predictive models, deep learning, neural network 2022Z\n",
    "\n",
    "# Reccurrent  neural networks (RNN)\n",
    "\n",
    "- first time RNN were developed in 1980\n",
    "- have connections that have loops\n",
    "- feedback is added to the networks over time\n",
    "- recurrent neural networks are in fact recursive neural network\n",
    "- memory allows to learn and generalize across sequences of inputs rather than individual patterns\n",
    "\n",
    "\n",
    "<img src=\"RNN1.png\">\n",
    "\n",
    "### RNN Types\n",
    "   - Hopfield Networks(1982)\n",
    "   - Bidirectional Recurrent Neural Networks (BRNN)\n",
    "   - Long Short-Term Memory Networks (LSTM)\n",
    "   - Gated Recurrent Unit (GRU)\n",
    "\n",
    "\n",
    "# Recursive neural networks (TreeNet)\n",
    "- highly useful for parsing natural scenes and language\n",
    "- created after applying the same set of weights recursively on the structured inputs\n",
    "- powerful in learning hierarchical, tree-like structure\n",
    "- they are hard to train\n",
    "\n",
    "<img src=\"RNN1a.png\">\n",
    "\n",
    "\n",
    "### Weakness of Traditional Methods to Time Series Forecasting\n",
    "\n",
    "- no support for missing or corrupt data \n",
    "- assumption of a linear relationship excludes more complex joint distributions\n",
    "- fixed temporal dependence: the number of lag observations provided as input is specified.\n",
    "- support for univariate data: many real-world problems have multiple input variables.\n",
    "- support for one-step forecasts: many real-world problems require forecasts with a long time horizon\n",
    "\n",
    "###  Sequences in Neural Networks\n",
    "\n",
    "Let's take the data set, which contains hourly energy consumption. This dataset can be framed as a prediction problem for a classical feedforward multilayer perceptron network by defining a windows size (e.g. 48) and training the network to learn to make short term predictions from the fixed sized window of inputs.\n",
    "\n",
    "Source: https://www.kaggle.com/robikscube/hourly-energy-consumption\n",
    "\n",
    "However this approach does not allow us to capture the <b>broader trends</b> that might be relevant to make a prediction.\n",
    "<br>\n",
    "<b>Solution:</b><br>\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a special type of neural network designed for sequence problems.\n",
    "A recurrent neural network can be thought of as the addition of loops to the architecture. Each neuron may pass its signal latterly (sideways) in addition to forward to the next layer. The output of the network may feedback as an input to the network with the next input vector.\n",
    "\n",
    "### Problem of Backpropagation algorithm\n",
    "\n",
    "Standard backpropagation algorithm  does not work properly in reccurent networks, because these networks contains loops.\n",
    "\n",
    "<br>\n",
    "<b>Solution</b> <br>\n",
    "New algorithm was presented: Backpropagation Through Time or BPTT. The new algorithm unrolls the network structure,by creating the copies of the neurons that have recurrent connections.\n",
    "It means that a single neuron with a connection to itself will be represented as two neurons with the same weight values. <br>\n",
    "This allows the cyclic graph of a recurrent neural network to be turned into an acyclic graph like a classic feed-forward neural network, and Backpropagation can be applied.\n",
    "\n",
    "### Problem of the unstable  gradients during training \n",
    "\n",
    "In very deep neural networks and in unrolled recurrent neural networks, the gradients become unstable. We can observe very large numbers called exploding gradients or very small numbers called the vanishing gradient problem. \n",
    "When these large numbers are used to update the weights in the network,they make the training process unstable and the network unreliable.\n",
    "<br>\n",
    "In multilayer Perceptron networks this problem is solved by use of Rectifier transfer function.<br>\n",
    "\n",
    "<b>ReLU:</b>\n",
    "- acts like a linear function\n",
    "- easier to train and less likely to saturate \n",
    "\n",
    "<img src=\"relu.png\">\n",
    "<img src=\"tanh.png\">\n",
    "<br>\n",
    "\n",
    "When the gradient value is very small then the change of the weight is not significant.\n",
    "Tanh is flat except for a very narrow range (that range being about -2 to 2). The derivative of the function is very small unless the input is in this narrow range, and this flat derivative makes it difficult to improve the weights through gradient descent. This problem gets worse as the model has more layers <b>vanishing gradient problem.</b> <br>\n",
    "\n",
    "### Limitations of RNN\n",
    "\n",
    "- Recurrent Neural Networks deals easily with short-term dependencies like below:<br> \n",
    "        The color of the grass is ____  \n",
    "The  result is correct while RNN does not have to remember what was said 3 sentence before.\n",
    "- However in the text as below the RNN probably returns the wrong answer:\n",
    "        When I was in high school I always loved to spend days on mathematical and financial tasks. \n",
    "        During this period I met my friend  Adam who was a great football player. \n",
    "        ........\n",
    "        In summary I think that Adam could be a great sportsmen in future,while I would like to work in a ...Bank..... \n",
    "- In order to return the correct answer the RNN should remember the context\n",
    "\n",
    "\n",
    "### Bidirectional Recurrent Neural Networks (BRNN)\n",
    "- introduced by Schuster and Paliwal (1997)\n",
    "- use information from the past and future to model current values\n",
    "- during training pahse use past and future data to estimate the present,but during the test phase only past data is available\n",
    "- require both a forward and a backward pass,gradients have long dependence chain -> learning process is slow \n",
    "- applications: \n",
    "    - speech recognition,\n",
    "    - translation,\n",
    "    - handwritten recognition\n",
    "    - part-of-speech tagging\n",
    "    - dependency parsing\n",
    "\n",
    "<img src=\"brnn.png\">\n",
    "\n",
    "### Long Short-Term Memory Networks\n",
    "<br>\n",
    "<b>History of LSTM</b>: <br>\n",
    "\n",
    "- first time proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber [https://www.researchgate.net/publication/13853244_Long_Short-term_Memory]\n",
    "- improved in 2000 by Felix Gers' team\n",
    "\n",
    "The Long Short-Term Memory or LSTM network is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem.\n",
    "<br>\n",
    "Instead of neurons, LSTM networks have memory blocks that are connected into layers. <br>\n",
    "\n",
    "\n",
    "#### LSTM Applications\n",
    "\n",
    "- handwriting recognition and generation\n",
    "- language modeling and translation\n",
    "- acoustic modeling of speech\n",
    "- speech synthesis\n",
    "- protein secondary structure prediction\n",
    "- analysis of audio and video data\n",
    "- robot control\n",
    "- music composition\n",
    "- airport passenger management\n",
    "- short-term traffic forecast\n",
    "\n",
    "#### Improvement over RNN\n",
    "- previous version of RNN in order to add a new information, it transforms the existing information completely by applying a transformation function. It does not recognize which input information is important and which is not\n",
    "- LSTMs can selectively remember or forget things\n",
    "\n",
    "### A memory block in LSTM\n",
    "\n",
    "A block contains gates that manage the block’s state and output. A unit operates upon an input sequence and each gate within a unit uses the sigmoid activation function to control whether they are triggered or not, making the change of state and addition of information flowing through the unit conditional.\n",
    "\n",
    "There are three types of gates within a memory unit:\n",
    "\n",
    "- Forget Gate: conditionally decides what information to discard from the unit.\n",
    "- Input Gate: conditionally decides which values from the input to update the memory state.\n",
    "- Output Gate: conditionally decides what to output based on input and the memory of the unit.\n",
    "\n",
    "#### Gate\n",
    "- decides which information should go through\n",
    "- composed out of a sigmoid neural net layer and a pointwise multiplication operation\n",
    "\n",
    "#### Forget gate\n",
    "\n",
    "- decides how to use information stored in memory\n",
    "- outputs a number between 0 and 1 for each number in the cell state with index t−1\n",
    "\n",
    "<b> Example:</b> Tom is a short kid. Adam on the other hand is a very tall person. <br>\n",
    "   As soon as the first full stop after “kid” is encountered, the forget gate realizes that there may be a change of context in the next sentence. <br>\n",
    "\n",
    "\n",
    "#### Input Gate\n",
    "\n",
    "- regulates what values need to be added to the cell state by involving a sigmoid function\n",
    "\n",
    "#### Output Gate\n",
    "\n",
    "- selects useful information from the current cell state and showing it out as an output\n",
    "\n",
    "\n",
    "<img src=\"lstm2.png\">\n",
    "\n",
    "<img src=\"LSTM1.png\">\n",
    "\n",
    "\n",
    "#### How it works?\n",
    "\n",
    "- the triple arrows at the bottom show where information flows into the cell at multiple points \n",
    "- combination of present input and past cell state is fed to the cell itself, but also to each of its three gates\n",
    "- all three gates decide how the input data will be handled\n",
    "- $s_c$ is the current state of the memory cell \n",
    "- $g y^{in} $ is the current input \n",
    "- each gate can be open or shut\n",
    "- each cell can forget its state, or not \n",
    "\n",
    "## Gated Recurrent Unit (GRU)\n",
    "- introduced in 2014 by Kyunghyun Cho \n",
    "- fewer parameters than LSTM\n",
    "- joins the forget and input gates into a single “update gate”\n",
    "- have only two gates: update gate and reset gate.\n",
    "- better performance on certain smaller datasets than LTSTM\n",
    "\n",
    "##### Update gate (z_t)\n",
    "- decides how much information from the past we want to reuse in future\n",
    "\n",
    "#### Reset gate (r_t)\n",
    "- decides how much information from the past we want forget \n",
    "\n",
    "<img src=\"gru.png\">\n",
    "\n",
    "### Neural Networks parameters\n",
    "\n",
    "##### Dropout\n",
    "Regularization method where input and recurrent connections to LSTM units are probabilistically excluded from activation and weight updates while training a network. This has the effect of reducing overfitting and improving model performance.\n",
    "\n",
    "###### Input dropout\n",
    "A dropout on the input means that for a given probability, the data on the input connection to each LSTM block will be excluded from node activation and weight updates.\n",
    "\n",
    "#### Recurrent dropout\n",
    "- allows to overcome the overfitting\n",
    "- the same dropout mask (the same pattern of dropped units) should be applied at every timestep\n",
    "\n",
    "<b>Remark:</b> <br>\n",
    "Applying standard dropout to RNN’s tends limits the ability of the networks to retain their memory, hindering their performance. \n",
    "\n",
    "##### Keras  dropout-related arguments\n",
    " - dropout = float specifying the dropout rate for input units of the layer\n",
    " - recurrent_dropout = specifying the dropout rate of the recurrent units\n",
    "\n",
    "##### Epoch\n",
    "\n",
    "- single pass of ENTIRE dataset forward and backward through the neural network\n",
    "\n",
    "##### Batch \n",
    "- number of training records which are used to calculate the cost function value and update the NN weights\n",
    "\n",
    "##### Itertations\n",
    "- number of batches needed to complete ONE epoch\n",
    "\n",
    "<br> <br>\n",
    "<b>Example:</b><br>\n",
    "- 1000 input records and batch size equal to 100 means 10 iterations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Articles:\n",
    "\n",
    "- https://link.springer.com/chapter/10.1007/3-540-44668-0_93\n",
    "- https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literature:\n",
    "\n",
    "1. Language Modeling and Generating Text\n",
    " - Recurrent neural network based language model <br><http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf>\n",
    "\n",
    " - Extensions of Recurrent neural network based language model\n",
    "<http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf>\n",
    "\n",
    "2. Machine Translation\n",
    " - A Recursive Recurrent Neural Network for Statistical Machine Translation<br>\n",
    "<http://www.aclweb.org/anthology/P14-1140.pdf>\n",
    " - Sequence to Sequence Learning with Neural Networks<br>\n",
    "<http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf>\n",
    " - Joint Language and Translation Modeling with Recurrent Neural Networks<br>\n",
    "<http://research.microsoft.com/en-us/um/people/gzweig/Pubs/EMNLP2013RNNMT.pdf>\n",
    "\n",
    "3. Speech Recognition\n",
    " - Towards End-to-End Speech Recognition with Recurrent Neural Networks<br>\n",
    "<http://www.jmlr.org/proceedings/papers/v32/graves14.pdf>\n",
    "\n",
    "4. Protein prediction\n",
    " - Exploiting_the_Past_and_the_Future_in_Protein_Secondary_Structure_Prediction<br>\n",
    "<https://www.researchgate.net/publication/12571895_Exploiting_the_Past_and_the_Future_in_Protein_Secondary_Structure_Prediction>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example:Predict some n number of characters after the original text of Christmas stories\n",
    "\n",
    "The text can be found here: https://www.gutenberg.org/files/1467/1467-0.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing dependencies numpy and keras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dropout\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Importing dependencies numpy and keras\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading text file and creating character to integer mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '(', ')', ',', '-', '.', '1', '2', '5', '8', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '»', 'â', 'ă', 'ď', 'ś', 'ť', 'ż', '”', '€', '™']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = \"christmasStories.txt\"\n",
    "\n",
    "text = (open(filename).read()).lower()\n",
    "\n",
    "# mapping characters with integers\n",
    "unique_chars = sorted(list(set(text)))\n",
    "print(unique_chars)\n",
    "\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for i, c in enumerate (unique_chars):\n",
    "    char_to_int.update({c: i})\n",
    "    int_to_char.update({i: c})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing input and output dataset\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(0, len(text) - 50, 1):\n",
    "    sequence = text[i:i + 50]\n",
    "    label =text[i + 50]\n",
    "    #print(sequence)\n",
    "    X.append([char_to_int[char] for char in sequence])\n",
    "    Y.append(char_to_int[label])\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping, normalizing and one hot encoding\n",
    "X_modified = numpy.reshape(X, (len(X), 50, 1))\n",
    "X_modified = X_modified / float(len(unique_chars))\n",
    "Y_modified = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Robert\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Robert\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Robert\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Robert\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Robert\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Robert\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Robert\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first layer is an LSTM layer with 300 memory units and it returns sequences\n",
    "- A dropout layer is applied after each LSTM layer to avoid overfitting of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model and generating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Robert\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/1\n",
      "62741/62741 [==============================] - 485s 8ms/step - loss: 2.9347\n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "e\n",
      " \n",
      "t\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "model.fit(X_modified, Y_modified, epochs=1, batch_size=30)\n",
    "\n",
    "# picking a random seed\n",
    "start_index = numpy.random.randint(0, len(X)-1)\n",
    "new_string = X[start_index]\n",
    "\n",
    "# generating characters\n",
    "for i in range(50):\n",
    "    x = numpy.reshape(new_string, (1, len(new_string), 1))\n",
    "    x = x / float(len(unique_chars))\n",
    "\n",
    "    #predicting\n",
    "    pred_index = numpy.argmax(model.predict(x, verbose=0))\n",
    "    char_out = int_to_char[pred_index]\n",
    "    seq_in = [int_to_char[value] for value in new_string]\n",
    "    print(char_out)\n",
    "\n",
    "    new_string.append(pred_index)\n",
    "    #remove the first character\n",
    "    new_string = new_string[1:len(new_string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
